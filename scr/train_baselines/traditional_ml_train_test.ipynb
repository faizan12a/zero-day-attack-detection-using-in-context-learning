{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_and_sample_data(dataset_folder, total_length, normal_ratio):\n",
    "    \"\"\"\n",
    "    Loads datasets, samples a specific ratio of normal data, and balances remaining among attack datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset_folder (str): Path to the folder containing all datasets.\n",
    "        total_length (int): Total length of the training dataset to be sampled.\n",
    "        normal_ratio (float): Ratio of normal samples in the dataset (0.0 to 1.0).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled data features.\n",
    "        torch.Tensor: Corresponding labels.\n",
    "    \"\"\"\n",
    "    # Identify dataset files\n",
    "    exclude_keywords = [\"randomreplay\", \"masqueradefakenormal\", \"masqueradefakefault\", \"poisonedhighrate\"]\n",
    "    dataset_files = [f for f in os.listdir(dataset_folder) if f.endswith('.npy') and not any(keyword in f.lower() for keyword in exclude_keywords)]\n",
    "    normal_file = [f for f in dataset_files if 'normal' in f.lower()]\n",
    "    attack_files = [f for f in dataset_files if 'normal' not in f.lower()]\n",
    "\n",
    "    if not normal_file:\n",
    "        raise ValueError(\"No normal dataset file found in the folder.\")\n",
    "\n",
    "    # Load the normal dataset\n",
    "    normal_data = np.load(os.path.join(dataset_folder, normal_file[0]))\n",
    "    \n",
    "    # Sample normal data\n",
    "    normal_length = int(total_length * normal_ratio)\n",
    "    if len(normal_data) < normal_length:\n",
    "        raise ValueError(f\"Not enough normal data: {len(normal_data)} samples available, {normal_length} required.\")\n",
    "    sampled_normal = normal_data[np.random.choice(len(normal_data), normal_length, replace=False)]\n",
    "    normal_labels = np.zeros(normal_length)  # Label for normal data is 0\n",
    "\n",
    "    # Load and sample attack data\n",
    "    attack_length_per_file = int((total_length - normal_length) / len(attack_files))\n",
    "    sampled_attack_data = []\n",
    "    sampled_attack_labels = []\n",
    "\n",
    "    for attack_file in attack_files:\n",
    "        attack_data = np.load(os.path.join(dataset_folder, attack_file))\n",
    "        if len(attack_data) < attack_length_per_file:\n",
    "            raise ValueError(f\"Not enough data in {attack_file}: {len(attack_data)} samples available, {attack_length_per_file} required.\")\n",
    "        sampled_attack = attack_data[np.random.choice(len(attack_data), attack_length_per_file, replace=False)]\n",
    "        sampled_attack_data.append(sampled_attack)\n",
    "        sampled_attack_labels.extend([1] * attack_length_per_file)  # Label for attack data is 1\n",
    "\n",
    "    # Combine normal and attack data\n",
    "    all_data = np.concatenate([sampled_normal] + sampled_attack_data, axis=0)\n",
    "    all_labels = np.concatenate([normal_labels, sampled_attack_labels], axis=0)\n",
    "\n",
    "    # Shuffle the combined dataset\n",
    "    shuffle_indices = np.random.permutation(len(all_data))\n",
    "    all_data = all_data[shuffle_indices]\n",
    "    all_labels = all_labels[shuffle_indices]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    return np.array(all_data, dtype=np.float32), np.array(all_labels, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Best parameters: {'C': 0.1}\n",
      "Best training accuracy: 1.000\n",
      "\n",
      "Decision Tree:\n",
      "Best parameters: {'max_depth': None, 'min_samples_split': 2}\n",
      "Best training accuracy: 1.000\n",
      "\n",
      "Random Forest:\n",
      "Best parameters: {'max_depth': None, 'n_estimators': 10}\n",
      "Best training accuracy: 1.000\n",
      "\n",
      "SVM:\n",
      "Best parameters: {'svc__C': 0.1, 'svc__gamma': 0.1}\n",
      "Best training accuracy: 0.985\n",
      "\n",
      "Naive Bayes:\n",
      "Best parameters: {'var_smoothing': 1e-09}\n",
      "Best training accuracy: 0.875\n",
      "\n",
      "The best model for Logistic Regression is: LogisticRegression(C=0.1, max_iter=100000)\n",
      "\n",
      "The best model for Decision Tree is: DecisionTreeClassifier()\n",
      "\n",
      "The best model for Random Forest is: RandomForestClassifier(n_estimators=10)\n",
      "\n",
      "The best model for SVM is: Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.1, gamma=0.1))])\n",
      "\n",
      "The best model for Naive Bayes is: GaussianNB()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_folder = \"/home/mfaizan/programs/my_project/data/weak_classifiers/training_preprocessed\"\n",
    "total_length = 100000\n",
    "normal_ratio = 0.5\n",
    "# Create synthetic dataset\n",
    "X, y = load_and_sample_data(dataset_folder, total_length, normal_ratio)\n",
    "    # Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define models and parameter grids\n",
    "models_params = {\n",
    "    # 'Logistic Regression': (LogisticRegression(max_iter=100000), {'C': [0.1, 1, 10, 100]}),\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=100000), {'C': [0.1]}),\n",
    "    # 'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None], 'min_samples_split': [2]}),\n",
    "    'Random Forest': (RandomForestClassifier(), {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}),\n",
    "    'SVM': (make_pipeline(StandardScaler(), SVC()), {'svc__C': [0.1], 'svc__gamma': [ 0.1]}),\n",
    "    'Naive Bayes': (GaussianNB(), {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]})\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "# Loop through models, perform GridSearchCV, and print out results\n",
    "for name, (model, params) in models_params.items():\n",
    "    grid = GridSearchCV(model, params, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Storing the best model\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Best parameters: {grid.best_params_}\")\n",
    "    print(f\"Best training accuracy: {grid.best_score_:.3f}\")\n",
    "    print()  # Print a blank line for readability\n",
    "\n",
    "# Now best_models dict contains the best model for each type\n",
    "for name, model in best_models.items():\n",
    "    print(f\"The best model for {name} is: {model}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for Logistic Regression:\n",
      "Goose inversereplay: 1.000\n",
      "Goose poisonedhighrate: 1.000\n",
      "Goose masqueradefakenormal: 0.102\n",
      "Goose randomreplay: 1.000\n",
      "Goose injection: 1.000\n",
      "Goose normal: 1.000\n",
      "Sv highstnum: 1.000\n",
      "Goose highstnum: 1.000\n",
      "Sv injection: 1.000\n",
      "Goose masqueradefakefault: 1.000\n",
      "\n",
      "Accuracies for Decision Tree:\n",
      "Goose inversereplay: 1.000\n",
      "Goose poisonedhighrate: 1.000\n",
      "Goose masqueradefakenormal: 0.000\n",
      "Goose randomreplay: 1.000\n",
      "Goose injection: 1.000\n",
      "Goose normal: 1.000\n",
      "Sv highstnum: 1.000\n",
      "Goose highstnum: 1.000\n",
      "Sv injection: 1.000\n",
      "Goose masqueradefakefault: 1.000\n",
      "\n",
      "Accuracies for Random Forest:\n",
      "Goose inversereplay: 1.000\n",
      "Goose poisonedhighrate: 1.000\n",
      "Goose masqueradefakenormal: 0.008\n",
      "Goose randomreplay: 0.993\n",
      "Goose injection: 1.000\n",
      "Goose normal: 1.000\n",
      "Sv highstnum: 1.000\n",
      "Goose highstnum: 1.000\n",
      "Sv injection: 1.000\n",
      "Goose masqueradefakefault: 0.859\n",
      "\n",
      "Accuracies for SVM:\n",
      "Goose inversereplay: 1.000\n",
      "Goose poisonedhighrate: 1.000\n",
      "Goose masqueradefakenormal: 0.091\n",
      "Goose randomreplay: 1.000\n",
      "Goose injection: 1.000\n",
      "Goose normal: 0.970\n",
      "Sv highstnum: 1.000\n",
      "Goose highstnum: 1.000\n",
      "Sv injection: 1.000\n",
      "Goose masqueradefakefault: 0.996\n",
      "\n",
      "Accuracies for Naive Bayes:\n",
      "Goose inversereplay: 0.814\n",
      "Goose poisonedhighrate: 0.976\n",
      "Goose masqueradefakenormal: 0.721\n",
      "Goose randomreplay: 0.000\n",
      "Goose injection: 0.280\n",
      "Goose normal: 0.934\n",
      "Sv highstnum: 1.000\n",
      "Goose highstnum: 1.000\n",
      "Sv injection: 1.000\n",
      "Goose masqueradefakefault: 0.410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_models_on_tests(best_models, X_test_list, y_test_list, names):\n",
    "    # Check if the lists are of the same length\n",
    "    if not (len(X_test_list) == len(y_test_list) == len(names)):\n",
    "        raise ValueError(\"All input lists must have the same length.\")\n",
    "    \n",
    "    # Dictionary to hold accuracy scores\n",
    "    accuracy_dict = {}\n",
    "\n",
    "    # Evaluate each model on each test set\n",
    "    for name, model in best_models.items():\n",
    "        accuracies = []\n",
    "        for X_test, y_test in zip(X_test_list, y_test_list):\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            accuracies.append(accuracy)\n",
    "        accuracy_dict[name] = accuracies\n",
    "\n",
    "    # Convert accuracy dictionary to DataFrame\n",
    "    accuracy_df = pd.DataFrame(accuracy_dict, index=names)\n",
    "    \n",
    "    # Save to CSV\n",
    "    # accuracy_df.to_csv('/home/mfaizan/programs/gpt2_powersystems/final_testings/base_code/basic_model_accuracies3.csv')\n",
    "    \n",
    "    return accuracy_df\n",
    "\n",
    "def load_and_label_data(directory):\n",
    "    data_list = []\n",
    "    y_list = []\n",
    "    headers = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.npy'):\n",
    "            # Load the numpy array\n",
    "            data = np.load(os.path.join(directory, file_name))\n",
    "            data_list.append(data)\n",
    "\n",
    "            # Generate header\n",
    "            headers.append(file_name.replace('_test.npy', '').replace('_', ' ').capitalize())\n",
    "\n",
    "            # Assign label based on 'normal' presence in file name\n",
    "            if 'normal' in file_name.lower():\n",
    "                y_list.append(np.zeros((len(data),1)))\n",
    "            else:\n",
    "                y_list.append(np.ones((len(data),1)))\n",
    "    \n",
    "    return data_list, y_list, headers\n",
    "\n",
    "# Example usage\n",
    "directory = '/home/mfaizan/programs/my_project/data/weak_classifiers/testing_preprocessed'  # Replace with your actual directory path\n",
    "Xtest_List, ytest_List, headers = load_and_label_data(directory)\n",
    "\n",
    "# Evaluate all best models on all test sets\n",
    "accuracy_results = evaluate_models_on_tests(best_models, Xtest_List, ytest_List, headers)\n",
    "\n",
    "# Print the accuracy results\n",
    "for model_name, accuracies in accuracy_results.items():\n",
    "    print(f\"Accuracies for {model_name}:\")\n",
    "    for test_name, accuracy in accuracies.items():\n",
    "        print(f\"{test_name}: {accuracy:.3f}\")\n",
    "    print()  # Print a blank line for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
